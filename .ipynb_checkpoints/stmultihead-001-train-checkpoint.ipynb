{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import yaml\n",
    "import copy\n",
    "import logging\n",
    "\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "# augmentation\n",
    "import albumentations\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform, DualTransform\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "#\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "import sys\n",
    "sys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\n",
    "import timm\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"\"\"\n",
    "globals:\n",
    "  seed: 107\n",
    "  device: cuda\n",
    "  max_epoch: 8\n",
    "  patience: 3\n",
    "  dataset_name: train_512x512\n",
    "  use_amp: True\n",
    "  val_fold: 0\n",
    "  debug: False\n",
    "  folds: [0, 1, 2, 3, 4]\n",
    "  classes: 11\n",
    "  fold1: True\n",
    "\n",
    "\n",
    "dataset:\n",
    "  name: LabeledImageDatasetNumpy\n",
    "  train:\n",
    "    transform_list:\n",
    "      - [HorizontalFlip, {p: 0.5}]\n",
    "      - [ShiftScaleRotate, {\n",
    "          p: 0.5, shift_limit: 0.2, scale_limit: 0.2,\n",
    "          rotate_limit: 20, border_mode: 0, value: 0, mask_value: 0}]\n",
    "      - [RandomResizedCrop, {height: 512, width: 512, scale: [0.9, 1.0]}]\n",
    "      - [Cutout, {max_h_size: 51, max_w_size: 51, num_holes: 5, p: 0.5}]\n",
    "      - [Normalize, {\n",
    "          always_apply: True, max_pixel_value: 255.0,\n",
    "          mean: [0.4887381077884414], std: [0.23064819430546407]}]\n",
    "      - [ToTensorV2, {always_apply: True}]\n",
    "  valid:\n",
    "    transform_list:\n",
    "      - [Normalize, {\n",
    "          always_apply: True, max_pixel_value: 255.0,\n",
    "          mean: [0.4887381077884414], std: [0.23064819430546407]}]\n",
    "      - [ToTensorV2, {always_apply: True}]\n",
    "\n",
    "loader:\n",
    "  train: {batch_size: 8, shuffle: True, num_workers: 2, pin_memory: True, drop_last: True}\n",
    "  valid: {batch_size: 16, shuffle: False, num_workers: 2, pin_memory: True, drop_last: False}\n",
    "\n",
    "model:\n",
    "  name: StMultiHeadModel\n",
    "  params:\n",
    "    base_name: regnety_032\n",
    "    out_dims_head: [3, 4, 3, 1]\n",
    "    pretrained: True\n",
    "    fmap_size: 512\n",
    "\n",
    "optimizer:\n",
    "    name: Adam\n",
    "    params:\n",
    "      lr: 1.0e-03\n",
    "\n",
    "scheduler:\n",
    "  name: CosineAnnealingWarmRestarts\n",
    "  params:\n",
    "    T_0: 8\n",
    "    T_mult: 1\n",
    "\n",
    "loss: {name: BCEWithLogitsLoss, params: {}}\n",
    "\n",
    "early_stopping:\n",
    "  params: {mode: min, patience: 10}\n",
    "\"\"\"\n",
    "\n",
    "config = yaml.safe_load(config)\n",
    "\n",
    "\n",
    "config['globals']['name'] = \"StMultiHead_001\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # # logging 設定\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    # log 標準出力\n",
    "    handler_st = logging.StreamHandler()\n",
    "    handler_st.setLevel(logging.DEBUG)\n",
    "    handler_st_format = logging.Formatter('%(asctime)s %(name)s: %(message)s')\n",
    "    handler_st.setFormatter(handler_st_format)\n",
    "    logger.addHandler(handler_st)\n",
    "\n",
    "    # log ファイル\n",
    "    log_file = f'log_{config[\"globals\"][\"name\"]}.log'\n",
    "    handler_f = logging.FileHandler(log_file, 'a')\n",
    "    handler_f.setLevel(logging.DEBUG)\n",
    "    handler_f_format = logging.Formatter('%(asctime)s %(name)s: %(message)s')\n",
    "    handler_f.setFormatter(handler_f_format)\n",
    "    logger.addHandler(handler_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path.cwd().parent\n",
    "INPUT = ROOT / \"input\"\n",
    "OUTPUT = ROOT / \"output\"\n",
    "DATA = INPUT / \"ranzcr-clip-catheter-line-classification\"\n",
    "TRAIN = DATA / \"train\"\n",
    "TEST = DATA / \"test\"\n",
    "\n",
    "\n",
    "TRAIN_NPY = INPUT / \"ranzcrcliptrainnumpy\"\n",
    "TMP = ROOT / \"tmp\"\n",
    "TMP.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "CLASSES = [\n",
    "    'ETT - Abnormal',\n",
    "    'ETT - Borderline',\n",
    "    'ETT - Normal',\n",
    "    'NGT - Abnormal',\n",
    "    'NGT - Borderline',\n",
    "    'NGT - Incompletely Imaged',\n",
    "    'NGT - Normal',\n",
    "    'CVC - Abnormal',\n",
    "    'CVC - Borderline',\n",
    "    'CVC - Normal',\n",
    "    'Swan Ganz Catheter Present'\n",
    "]\n",
    "\n",
    "\n",
    "RANDAM_SEED = 107\n",
    "N_CLASSES = 11\n",
    "FOLDS = [0, 1, 2, 3, 4]\n",
    "N_FOLD = len(FOLDS)\n",
    "\n",
    "train = pd.read_csv(DATA / \"train.csv\")\n",
    "smpl_sub = pd.read_csv(DATA / \"sample_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_arr_320 = save_as_numpy(TEST_RESIZED.resolve(), TMP / \"test_{0}x{1}.npy\".format(*IMAGE_SIZE), test, IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def multi_label_stratified_group_k_fold(label_arr: np.array, gid_arr: np.array, n_fold: int, seed: int = 107):\n",
    "    \"\"\"\n",
    "    create multi-label stratified group kfold indexs.\n",
    "\n",
    "    reference: https://www.kaggle.com/jakubwasikowski/stratified-group-k-fold-cross-validation\n",
    "    input:\n",
    "        label_arr: numpy.ndarray, shape = (n_train, n_class)\n",
    "            multi-label for each sample's index using multi-hot vectors\n",
    "        gid_arr: numpy.array, shape = (n_train,)\n",
    "            group id for each sample's index\n",
    "        n_fold: int. number of fold.\n",
    "        seed: random seed.\n",
    "    output:\n",
    "        yield indexs array list for each fold's train and validation.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    start_time = time.time()\n",
    "    n_train, n_class = label_arr.shape\n",
    "    gid_unique = sorted(set(gid_arr))\n",
    "    n_group = len(gid_unique)\n",
    "\n",
    "    # # aid_arr: (n_train,), indicates alternative id for group id.\n",
    "    # # generally, group ids are not 0-index and continuous or not integer.\n",
    "    gid2aid = dict(zip(gid_unique, range(n_group)))\n",
    "    # aid2gid = dict(zip(range(n_group), gid_unique))\n",
    "    aid_arr = np.vectorize(lambda x: gid2aid[x])(gid_arr)\n",
    "\n",
    "    # # count labels by class\n",
    "    cnts_by_class = label_arr.sum(axis=0)  # (n_class, )\n",
    "\n",
    "    # # count labels by group id.\n",
    "    col, row = np.array(sorted(enumerate(aid_arr), key=lambda x: x[1])).T\n",
    "    cnts_by_group = coo_matrix(\n",
    "        (np.ones(len(label_arr)), (row, col))\n",
    "    ).dot(coo_matrix(label_arr)).toarray().astype(int)\n",
    "    del col\n",
    "    del row\n",
    "    cnts_by_fold = np.zeros((n_fold, n_class), int)\n",
    "\n",
    "    groups_by_fold = [[] for _ in range(n_fold)]\n",
    "    group_and_cnts = list(enumerate(cnts_by_group))  # pair of aid and cnt by group\n",
    "    np.random.shuffle(group_and_cnts)\n",
    "    logger.debug(f'finished preparation {time.time() - start_time}')\n",
    "\n",
    "    for aid, cnt_by_g in sorted(group_and_cnts, key=lambda x: -np.std(x[1])):\n",
    "        best_fold = None\n",
    "        min_eval = None\n",
    "        for fid in range(n_fold):\n",
    "            # # eval assignment.\n",
    "            cnts_by_fold[fid] += cnt_by_g\n",
    "            fold_eval = (cnts_by_fold / cnts_by_class).std(axis=0).mean()\n",
    "            cnts_by_fold[fid] -= cnt_by_g\n",
    "\n",
    "            if min_eval is None or fold_eval < min_eval:\n",
    "                min_eval = fold_eval\n",
    "                best_fold = fid\n",
    "\n",
    "        cnts_by_fold[best_fold] += cnt_by_g\n",
    "        groups_by_fold[best_fold].append(aid)\n",
    "    logger.debug(f'finished assignment {time.time() - start_time}')\n",
    "\n",
    "    gc.collect()\n",
    "    idx_arr = np.arange(n_train)\n",
    "    for fid in range(n_fold):\n",
    "        val_groups = groups_by_fold[fid]\n",
    "\n",
    "        val_indexs_bool = np.isin(aid_arr, val_groups)\n",
    "        train_indexs = idx_arr[~val_indexs_bool]\n",
    "        val_indexs = idx_arr[val_indexs_bool]\n",
    "\n",
    "        logger.info(f'[fold {fid}]')\n",
    "        logger.info(f'n_group: (train, val) = ({n_group - len(val_groups)} , {len(val_groups)})')\n",
    "        logger.info(f'n_sample: (train. val) = ({len(train_indexs), len(val_indexs)})')\n",
    "\n",
    "        yield train_indexs, val_indexs\n",
    "\n",
    "\n",
    "def set_seed(seed, deterministic=False):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = deterministic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as tp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def get_activation(activation_name: str='relu'):\n",
    "    act_dict = {\n",
    "        'relu': nn.ReLU(inplace=True),\n",
    "        'tanh': nn.Tanh(),\n",
    "        'sigmoid': nn.Sigmoid()\n",
    "    }\n",
    "    if activation_name in act_dict:\n",
    "        return act_dict[activation_name]\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.avgpool2d = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear(in_channels, in_channels // reduction, bias=False)\n",
    "        self.fc2 = nn.Linear(in_channels // reduction, in_channels, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.siz()  # x: [B, C, H, W]\n",
    "\n",
    "        s = self.avgpool2d(x)  # s: [B, C, 1, 1]\n",
    "        s = torch.flatten(s, 1)  # s: [B, C, 1]\n",
    "        s = self.fc1(s)  # s: [B, C // reduction, 1]\n",
    "        s = F.relu(s, inplace=True)\n",
    "        s = self.fc2(s)  # s: [B, C, 1]\n",
    "        s = self.sigmoid(s)\n",
    "        s = s.view(b, c, 1, 1)  # S:[B, C, 1, 1]\n",
    "\n",
    "        x = x * s\n",
    "        return x\n",
    "\n",
    "\n",
    "class Conv2dBNActive(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int,\n",
    "            kernel_size: int, stride: int = 1, padding: int = 0,\n",
    "            bias: bool = False, use_bn: bool = True, active: str = \"relu\"):\n",
    "        super().__init__()\n",
    "        self.use_bn = use_bn\n",
    "\n",
    "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = get_activation(active)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv2d(x)\n",
    "        if self.use_bn:\n",
    "            x = self.bn(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SpatialAttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels_list: tp.List[int]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_layers = len(out_channels_list)\n",
    "        channels_list = [in_channels] + out_channels_list\n",
    "\n",
    "        assert self.n_layers > 0\n",
    "        assert channels_list[-1] == 1  # mask is 1ch\n",
    "\n",
    "        for i in range(self.n_layers - 1):\n",
    "            in_chs, out_chs = channels_list[i: i + 2]\n",
    "            layer = Conv2dBNActive(in_chs, out_chs, 3, 1, 1, active='relu')\n",
    "            setattr(self, f'conv{i + 1}', layer)\n",
    "\n",
    "        in_chs, out_chs = channels_list[-2:]\n",
    "        layer = Conv2dBNActive(in_chs, out_chs, 3, 1, 1, active='sigmoid')\n",
    "        setattr(self, f'conv{self.n_layers}', layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        for i in range(self.n_layers):\n",
    "            h = getattr(self, f'conv{i + 1}')(h)\n",
    "        h = h * x\n",
    "        return h\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv2d = nn.Conv2d(in_channels, 1, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = self.conv2d(x)\n",
    "        mask = self.activation(mask)\n",
    "\n",
    "        return mask\n",
    "\n",
    "\n",
    "class SpatialTransformer(nn.Module):\n",
    "    # https://pytorch.org/tutorials/intermediate/spatial_transformer_tutorial.html\n",
    "    def __init__(self, in_channels: int, fmap_size: int):\n",
    "        super().__init__()\n",
    "        self.fmap_size = fmap_size\n",
    "\n",
    "        self.conv1x1 = nn.Conv2d(in_channels, 1, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)\n",
    "\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=7, padding=3),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 10, kernel_size=5, padding=2),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        # loc\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(10 * int(self.fmap_size / 4) ** 2, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "\n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1x1(x)\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 10 * int(self.fmap_size / 4) ** 2)\n",
    "\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import typing as tp\n",
    "# # from parts import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torchsummary import summary\n",
    "\n",
    "\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, base_name: str='resnext50_32x4d', out_dim: int=11, pretrained: bool=False):\n",
    "        super().__init__()\n",
    "        self.base_name = base_name\n",
    "\n",
    "        base_model = timm.create_model(base_name, pretrained=pretrained)\n",
    "        fc_in_features = base_model.fc.in_features\n",
    "\n",
    "        base_model.reset_classifier(0)\n",
    "\n",
    "        self.backbone = base_model\n",
    "\n",
    "        self.head_fc = nn.Sequential(\n",
    "            nn.Linear(fc_in_features, fc_in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(fc_in_features, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.backbone(x)\n",
    "        h = self.head_fc(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, base_name: str='resnext50_32x4d', out_dims: tp.List[int] = [3, 4, 3, 1], pretrained: bool=False):\n",
    "        super().__init__()\n",
    "        self.base_name = base_name\n",
    "        self.n_heads = len(out_dims)\n",
    "\n",
    "        base_model = timm.create_model(base_name, pretrained=pretrained)\n",
    "        fc_in_features = base_model.fc.in_features\n",
    "\n",
    "        # base_model.reset_classifier(0)\n",
    "        # base_model.reset_classifier(0, '')\n",
    "        base_model.fc = nn.Identity()\n",
    "        base_model.global_pool = nn.Identity()\n",
    "\n",
    "        self.backbone = base_model\n",
    "\n",
    "        for i, out_dim in enumerate(out_dims):\n",
    "            layer = nn.Sequential(\n",
    "                SpatialAttentionBlock(in_channels=fc_in_features, out_channels_list=[64, 32, 16, 1]),\n",
    "                nn.AdaptiveAvgPool2d(output_size=1),\n",
    "                nn.Flatten(start_dim=1),\n",
    "                nn.Linear(fc_in_features, fc_in_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(fc_in_features, fc_in_features)\n",
    "            )\n",
    "            setattr(self, f'head_{i}', layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = [getattr(self, f'head_{i}')(x) for i in range(self.n_heads)]\n",
    "        x = torch.cat(x, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, base_name: str = 'resnext50_32x4d',\n",
    "            out_dims_head: tp.List[int] = [3, 4, 3, 1], pretrained=False):\n",
    "        \"\"\"\"\"\"\n",
    "        self.base_name = base_name\n",
    "        self.n_heads = len(out_dims_head)\n",
    "        super(MultiHeadModel, self).__init__()\n",
    "\n",
    "        # # load base model\n",
    "        base_model = timm.create_model(base_name, pretrained=pretrained)\n",
    "        in_features = base_model.num_features\n",
    "\n",
    "        # # remove global pooling and head classifier\n",
    "        base_model.reset_classifier(0, '')\n",
    "\n",
    "        # # Shared CNN Bacbone\n",
    "        self.backbone = base_model\n",
    "\n",
    "        # # Multi Heads.\n",
    "        for i, out_dim in enumerate(out_dims_head):\n",
    "            layer_name = f\"head_{i}\"\n",
    "            layer = nn.Sequential(\n",
    "                SpatialAttentionBlock(in_features, [64, 32, 16, 1]),\n",
    "                nn.AdaptiveAvgPool2d(output_size=1),\n",
    "                nn.Flatten(start_dim=1),\n",
    "                nn.Linear(in_features, in_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(in_features, out_dim))\n",
    "            setattr(self, layer_name, layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\"\"\"\n",
    "        h = self.backbone(x)\n",
    "        hs = [\n",
    "            getattr(self, f\"head_{i}\")(h) for i in range(self.n_heads)]\n",
    "        y = torch.cat(hs, axis=1)\n",
    "        return y\n",
    "    \n",
    "\n",
    "class StMultiHeadModel(nn.Module):\n",
    "    def __init__(self, base_name: str = 'resnext50_32x4d', out_dims_head: tp.List[int] = [3, 4, 3, 1],\n",
    "                 pretrained=False, fmap_size: int = 256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.multiheadmodel = MultiHeadModel(base_name, out_dims_head, pretrained)\n",
    "\n",
    "        self.stn = SpatialTransformer(in_channels=3, fmap_size=fmap_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stn(x)\n",
    "        x = x.repeat(1, 3, 1, 1)\n",
    "        x = self.multiheadmodel(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImageTransformBase:\n",
    "    \"\"\"\n",
    "    Base Image Transform class.\n",
    "\n",
    "    Args:\n",
    "        data_augmentations: List of tuple(method: str, params :dict), each elems pass to albumentations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_augmentations: List[Tuple[str, Dict]]):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        augmentations_list = [\n",
    "            self._get_augmentation(aug_name)(**params)\n",
    "            for aug_name, params in data_augmentations]\n",
    "        self.data_aug = albumentations.Compose(augmentations_list)\n",
    "\n",
    "    def __call__(self, pair: Tuple[np.ndarray]) -> Tuple[np.ndarray]:\n",
    "        \"\"\"You have to implement this by task\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_augmentation(aug_name: str) -> ImageOnlyTransform or DualTransform:\n",
    "        \"\"\"Get augmentations from albumentations\"\"\"\n",
    "        if hasattr(albumentations, aug_name):\n",
    "            return getattr(albumentations, aug_name)\n",
    "        else:\n",
    "            return eval(aug_name)\n",
    "\n",
    "\n",
    "class ImageTransformForCls(ImageTransformBase):\n",
    "    \"\"\"Data Augmentor for Classification Task.\"\"\"\n",
    "\n",
    "    def __init__(self, data_augmentations: List[Tuple[str, Dict]]):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super(ImageTransformForCls, self).__init__(data_augmentations)\n",
    "\n",
    "    def __call__(self, in_arrs: Tuple[np.ndarray, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Apply Transform.\"\"\"\n",
    "        img, label = in_arrs\n",
    "        augmented = self.data_aug(image=img)\n",
    "        img = augmented[\"image\"]\n",
    "\n",
    "        return img, label\n",
    "\n",
    "\n",
    "class LabeledImageDatasetNumpy(Dataset):\n",
    "    def __init__(self, file_list, transform_list, copy_in_channels=True, in_channels=3):\n",
    "        self.file_list = file_list\n",
    "        self.transform = ImageTransformForCls(transform_list)\n",
    "        self.copy_in_channels = copy_in_channels\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, label = self.file_list[index]\n",
    "\n",
    "        if img.shape[-1] == 2:\n",
    "            img = img[..., None]\n",
    "\n",
    "        if self.copy_in_channels:\n",
    "            img = np.repeat(img, self.in_channels, axis=2)\n",
    "\n",
    "        img, label = self.transform((img, label))\n",
    "        return img, label\n",
    "\n",
    "class LabeledImageDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for (image, label) pairs\n",
    "\n",
    "    reads images and applys transforms to them.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    file_list : List[Tuple[tp.Union[str, Path], tp.Union[int, float, np.ndarray]]]\n",
    "        list of (image file, label) pair\n",
    "    transform_list : List[Dict]\n",
    "        list of dict representing image transform \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_list: tp.List[\n",
    "            tp.Tuple[tp.Union[str, Path], tp.Union[int, float, np.ndarray]]],\n",
    "        transform_list: tp.List[tp.Dict],\n",
    "    ):\n",
    "        \"\"\"Initialize\"\"\"\n",
    "        self.file_list = file_list\n",
    "        self.transform = ImageTransformForCls(transform_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return Num of Images.\"\"\"\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return transformed image and mask for given index.\"\"\"\n",
    "        img_path, label = self.file_list[index]\n",
    "        img = self._read_image_as_array(img_path)\n",
    "        \n",
    "        img, label = self.transform((img, label))\n",
    "        return img, label\n",
    "\n",
    "    def _read_image_as_array(self, path: str):\n",
    "        \"\"\"Read image file and convert into numpy.ndarray\"\"\"\n",
    "        img_arr = cv2.imread(str(path))\n",
    "        img_arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n",
    "        return img_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders_for_inference(\n",
    "    file_list: tp.List[tp.List], batch_size=64,\n",
    "):\n",
    "    \"\"\"Create DataLoader\"\"\"\n",
    "    dataset = LabeledImageDataset(\n",
    "        file_list,\n",
    "        transform_list=[\n",
    "          [\"Normalize\", {\n",
    "              \"always_apply\": True, \"max_pixel_value\": 255.0,\n",
    "              \"mean\": [\"0.4887381077884414\"], \"std\": [\"0.23064819430546407\"]}],\n",
    "          [\"ToTensorV2\", {\"always_apply\": True}],\n",
    "        ])\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size, shuffle=False,\n",
    "        num_workers=2, pin_memory=True,\n",
    "        drop_last=False)\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "\n",
    "# test_file_list = [\n",
    "#     (test_dir / f\"{img_id}.png\", [-1] * 11)\n",
    "#     for img_id in smpl_sub[\"StudyInstanceUID\"].values]\n",
    "# test_loader = get_dataloaders_for_inference(test_file_list, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_fold(config, train_all, temp_path, print_progress=False):\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    device = torch.device(config['globals']['device'])\n",
    "    set_seed(config['globals']['seed'])\n",
    "\n",
    "    # read train, valid image files\n",
    "    valid_fold = config['globals']['valid_fold']\n",
    "    valid_idx = train_all[train_all['fold'] == valid_fold].index.values\n",
    "    train_idx = train_all[train_all['fold'] != valid_fold].index.values\n",
    "    if config['globals']['debug']:\n",
    "        train_idx = train_idx[:len(train_idx) // 20]\n",
    "\n",
    "    data_path = TRAIN_NPY / f'{config[\"globals\"][\"dataset_name\"]}.npy'\n",
    "\n",
    "    image_arrs = np.load(str(data_path), mmap_mode='r')\n",
    "    label_arr = train_all[CLASSES].values.astype('f')\n",
    "\n",
    "    # Data set\n",
    "    train_arr_list = [(image_arrs[idx][:, :, None], label_arr[idx]) for idx in train_idx]\n",
    "    valid_arr_list = [(image_arrs[idx][:, :, None], label_arr[idx]) for idx in valid_idx]\n",
    "\n",
    "    train_dataset = LabeledImageDatasetNumpy(train_arr_list, **config['dataset']['train'])\n",
    "    valid_dataset = LabeledImageDatasetNumpy(valid_arr_list, **config['dataset']['valid'])\n",
    "    logger.debug(f'train_dataset: {len(train_dataset)}')\n",
    "    logger.debug(f'valid_dataset: {len(valid_dataset)}')\n",
    "\n",
    "    # DataLoader\n",
    "    train_loader = DataLoader(train_dataset, **config['loader']['train'])\n",
    "    valid_loader = DataLoader(valid_dataset, **config['loader']['valid'])\n",
    "\n",
    "    # model\n",
    "    model = eval(config['model']['name'])(**config['model']['params'])\n",
    "    # model = MultiHeadModel(**config['model']['params'])\n",
    "    model.to(device)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = getattr(torch.optim, config['optimizer']['name'])(model.parameters(), **config['optimizer']['params'])\n",
    "\n",
    "    # scheduler\n",
    "    if config['scheduler']['name'] == 'OneCycleLR':\n",
    "        config['scheduler']['params']['epochs'] = config['globals']['max_epoch']\n",
    "        config['scheduler']['params']['step_per_epoch'] = len(train_loader)\n",
    "    scheduler = getattr(torch.optim.lr_scheduler, config['scheduler']['name'])(optimizer, **config['scheduler']['params'])\n",
    "\n",
    "    # loss\n",
    "    if hasattr(nn, config['loss']['name']):\n",
    "        loss_func = getattr(nn, config['loss']['name'])(**config['loss']['params'])\n",
    "    else:\n",
    "        loss_func = eval(config['loss']['name'])(**config['loss']['params'])\n",
    "    loss_func.to(device)\n",
    "\n",
    "    # Early stopping\n",
    "    early_stop = EarlyStopping(**config['early_stopping']['params'])\n",
    "\n",
    "    # Train Loop\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    iteration = 0\n",
    "    for epoch in range(config['globals']['max_epoch']):\n",
    "        logger.info(f'epoch {epoch + 1} / {config[\"globals\"][\"max_epoch\"]}')\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        scaler = torch.cuda.amp.GradScaler()  # ここかえた amp\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            iteration += 1\n",
    "            # ToDo 慶か時間を記録する\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.cuda.amp.autocast():  # ここかえた amp\n",
    "                y = model(images)\n",
    "                loss = loss_func(y, labels)\n",
    "            running_loss += loss\n",
    "            \n",
    "            scaler.scale(loss).backward()  # ここかえた amp\n",
    "            scaler.step(optimizer)  # ここかえた amp\n",
    "            scaler.update()  # ここかえた amp\n",
    "#             loss.backward()  # ここかえた amp\n",
    "#             optimizer.step()  # ここかえた amp\n",
    "\n",
    "            del loss  # 計算グラフの削除によるメモリ節約\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        logger.info(f'lr: {scheduler.get_last_lr()[0]}')\n",
    "        logger.info(f'train loss: {train_losses[-1]:.8f}')\n",
    "        logger.info(f'iteration: {iteration}')\n",
    "        scheduler.step()\n",
    "\n",
    "        # evaluation\n",
    "\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valid_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                y = model(images)\n",
    "\n",
    "                loss = loss_func(y, labels)\n",
    "                running_loss += loss\n",
    "\n",
    "                del loss\n",
    "\n",
    "            valid_losses.append(running_loss / len(valid_loader))\n",
    "            logger.info(f'valid loss: {valid_losses[-1]:.8f}')\n",
    "\n",
    "        # save model\n",
    "#         torch.save(model.state_dict(), f'models_trained/{config[\"globals\"][\"name\"]}_epoch{epoch + 1}.pth')  # ここ変えた\n",
    "        torch.save(model.state_dict(), f'{config[\"globals\"][\"name\"]}_epoch{epoch + 1}.pth')\n",
    "\n",
    "        # ToDo パラメータを逐次保存しもっともよいパラメータを呼び出すように変更する\n",
    "        # ToDo 保存したloss がオブジェクトになっているので改善する\n",
    "\n",
    "        # early stopping\n",
    "        if early_stop.step(valid_losses[-1]):\n",
    "            break\n",
    "\n",
    "    epochs = [i + 1 for i in range(len(train_losses))]\n",
    "    eval_df = pd.DataFrame(index=epochs, columns=['train_eval', 'valid_eval'])\n",
    "    eval_df['train_eval'] = train_losses\n",
    "    eval_df['valid_eval'] = valid_losses\n",
    "    eval_df.to_csv(f'{config[\"globals\"][\"name\"]}_eval.csv')\n",
    "\n",
    "\n",
    "class EarlyStopping(object):\n",
    "    def __init__(self, mode='min', min_delta=0, patience=10, percentage=False, on=False):\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = 0\n",
    "        self.is_better = None\n",
    "        self._init_is_better(mode, min_delta, percentage)\n",
    "        self.on = on\n",
    "\n",
    "        if patience == 0:\n",
    "            self.is_better = lambda a, b: True\n",
    "            self.step = lambda a: False\n",
    "\n",
    "    def step(self, metrics):\n",
    "        if self.best is None:\n",
    "            self.best = metrics\n",
    "            return False\n",
    "\n",
    "        if torch.isnan(metrics):\n",
    "            return True\n",
    "\n",
    "        if self.is_better(metrics, self.best):\n",
    "            self.num_bad_epochs = 0\n",
    "            self.best = metrics\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _init_is_better(self, mode, min_delta, percentage):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError('mode ' + mode + ' is unknown!')\n",
    "        if not percentage:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - min_delta\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + min_delta\n",
    "        else:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - (\n",
    "                            best * min_delta / 100)\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + (\n",
    "                            best * min_delta / 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "#     parser = argparse.ArgumentParser(description='pytorch runner')\n",
    "#     parser.add_argument('config_file', help='実行時の設定yamlファイルの読み込み')\n",
    "\n",
    "#     args = parser.parse_args()\n",
    "#     config_file = Path(args.config_file)\n",
    "\n",
    "    # read config file\n",
    "#     with open(config_file, 'r') as f:\n",
    "#         config = yaml.safe_load(f)\n",
    "\n",
    "    folds = config['globals']['folds']\n",
    "    n_fold = len(folds)\n",
    "    n_classes = config['globals']['classes']\n",
    "    seed = config['globals']['seed']\n",
    "    \n",
    "    if config['globals']['debug']:\n",
    "        config['globals']['max_epoch'] = 1\n",
    "        print('!' * 10, 'debug model', '!' * 10)\n",
    "\n",
    "\n",
    "    # make fold data\n",
    "    label_arr = train[CLASSES].values\n",
    "    patient_id = train.PatientID.values\n",
    "\n",
    "    train_valid_indexs = list(multi_label_stratified_group_k_fold(label_arr, patient_id, n_fold=n_fold, seed=seed))\n",
    "\n",
    "    train['fold'] = -1\n",
    "    for fold_id, (train_idx, valid_idx) in enumerate(train_valid_indexs):\n",
    "        train.loc[valid_idx, 'fold'] = fold_id\n",
    "\n",
    "    configs = []\n",
    "    for fold_id in range(n_fold):\n",
    "        temp_config = copy.deepcopy(config)\n",
    "        temp_config['globals']['valid_fold'] = fold_id\n",
    "        temp_config['globals']['name'] += f'_fold{fold_id:02d}'\n",
    "        configs.append(temp_config)\n",
    "\n",
    "        if config['globals']['debug']:\n",
    "            break\n",
    "\n",
    "        if config['globals']['fold1']:\n",
    "            break\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    for fold_id, config_fold in enumerate(configs):\n",
    "        logger.info(f'start train fold : {fold_id}')\n",
    "        train_one_fold(config_fold, train, TMP/f'fold{fold_id}', False)\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
